---
title: "Exploring potential gender stereotypes in the distributional semantics of child-directed speech"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Benjamin E. deMayo (bdemayo@princeton.edu)} \\ Princeton University}

abstract: >
   Abstract: In three analyses, I explore whether gender stereotypes might be present in the distributional semantics of the CHILDES corpus (MacWhinney 2000), a large compendium of transcribed conversations between caregivers and their children, by training 2 commonly-used word embedding models on the corpus. In the first analysis, I examine the correlation between the gender valence of individual word vector representations in the two word-embedding models. In the second analysis, I relate gender valence in the word vector representations of individual words to human ratings of those words' gender valence. In the third analysis, I examine whether specific stereotypical associations with gender are detectable in the vector space representation of the words. 
    
keywords: >
    gender bias; word-embedding models; distributional semantics.
    
output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(childesr)
library(text2vec)
library(word2vec)
library(viridis)
library(tidyverse)
library(kableExtra)

project_root <- here::here()
```

# Introduction 

Gender is a highly salient social category that develops within the first few years of life and maintains its importance across the lifespan [@ruble2006gender]. Gender stereotypes, characteristics that are believed to be true of a gender category as a whole, are most rigid in middle childhood (around ages 5-8), but their developmental roots are in early childhood [@halim2010gender]. How children form concepts of gender, as well as the stereotypes that are linked to those concepts, has long been a subject of research, with some researchers proposing that language input to children could have a consequential impact on children’s beliefs about gender categories. 

Broadly speaking, two theoretical approaches have attempted to explain how language input to children might shape their gender stereotypes. One approach has emphasized the communication of knowledge from adults to children in a “top-down” fashion, in which children hear statements that explicitly communicate information about groups, such as generic statements [e.g., “girls are good at reading”; Gelman, Ware, & Kleinberg, -@gelman2010effects]. Another approach, which I focus on here, emphasizes how children could pick up on subtle cues about gender concepts and stereotypes solely from the statistics of their language input. In other words, children could learn that words corresponding to particular activities, traits, occupations, and other characteristics are themselves gendered by virtue of the other words with which they co-occur. This latter approach shares an intimate link with the computational linguistic subfield of distributional semantics, which seeks to characterize how the meaning of linguistic items is related to how those items are distributed in large bodies of text. 

Several studies have leveraged the tools of distributional semantics to examine whether gender stereotypes are appreciable in natural language corpora; some of these studies focus specifically on language that would likely be heard by children. The general strategy used by these studies has involved taking large bodies of text [usually those with several million tokens, though this has not always been the case; @lewis2020might] and using them to train word embedding models, which generate representations of individual word types in a high-dimensional vector space based on each word type’s co-occurrence with other types. The key assumption in such a strategy is that words which frequently co-occur will have similar meanings. Once vector representations of words are obtained, cosine distances between individual lexical items in the vector space are calculated as a proxy of semantic similarity, allowing researchers to examine whether words’ vector representations show patterns of similarity that might be expected given prevalent societal stereotypes (e.g., that the word “doll” is closer to the word “girl” than it is to the word “boy”). This general analytic framework has been used to argue that gender stereotypes are present in the distributional structure of large bodies of naturalistic text, including web-based corpora, children’s books, and transcripts of films and television shows [@caliskan2017semantics; @charlesworth2021gender; @lewis2020gender; @bhatia2021changes].

In this work, I extend prior findings by examining the human-like gender-stereotypical biases that might emerge in the vector representations obtained from training word embedding models on a body of child-directed speech. Specifically, I use transcripts between caregivers and children between the ages of 1 and 3 years old from the North American English corpora in the Child Language Data Exchange System [CHILDES; MacWhinney, -@macwhinney2000childes] and extract vector-space semantic representations using 2 commonly-used word embedding models, Word2Vec [@mikolov2013efficient] and GloVe [@pennington2014glove].

# Method

## Data preprocessing

Child-directed language was sourced from all of the North American English transcripts in the CHILDES corpus [@macwhinney2000childes]. Transcripts were obtained using the `childes-db` API, which allows researchers to access utterances in a tabular format that includes metadata about each utterance, including its speaker's role (parent, grandparent, child, etc.), the gender of the child in the conversation, and the lemmatized "stem" of the utterance [@sanchez2019childes]. From this tabular data, the stems of utterances from mothers, fathers, grandparents and adults were extracted and concatenated to create the training data, which contained text from conversations to 1,156 children, and was comprised of 6,824,349 word tokens. 

## Word embedding model training

Two common word embedding models were used to obtain vector-space representations for words in CHILDES.  The first was Word2Vec [@mikolov2013efficient], which uses a 2-layer neural network to predict a given word in a sentence given its surrounding words (continuous bag of words approach, CBOW) or vice versa (skip-gram approach) and derives vector-space representations of each word based on the neural network weights between the input layer and the single hidden layer of the network. The second was GloVe [@pennington2014glove], an unsupervised learning algorithm which takes as input a sparse matrix encoding the co-occurrence frequency of each pair of lexical items in a corpus, and which learns vector representations for these items, such that the inner product between two vectors closely approximates a logarithmic transformation of the probability that those two lexical items co-occur in the text. For our purposes here, the two techniques have the same goal of extracting vector representations of words that are semantically meaningful, even though GloVe's learning strategy emphasizes global co-occurrence probability between pairs of words more than Word2Vec, which can be seen as centering more on the local semantic contexts that words appear in. In the following analyses, the context window of each word embedding model is set to 5 words in both directions from a target word, and word representations derived from GloVe and Word2Vec are vectors in a 50-dimensional space.

# Analyses

## Analysis 1: Broad comparison between Word2Vec and GloVe

The first, and most broad, analysis is a coarse indication of whether Word2Vec and GloVe are capturing roughly similar semantic information for words in the CHILDES corpus, specifically as it concerns individual words' gender valence. Here, I define *gender valence* as the degree which a word is either semantically closer to the concept 'boy' versus the concept 'girl'. This conceptualization of gender valence has a intuitive implementation with word embeddings: gender valence can be thought of as a word's average cosine similarity to one or more "anchor" words representing the concept 'girl', minus the word's similarity to one or more "anchor" words representing the concept 'boy'. For this analysis and those subsequent, anchor words were borrowed from @lewis2020might and were as follows: $\text{girl words} = \{{\text{girl, woman, sister, she, her, daughter}}\}$; $\text{boy words} = \{{\text{boy, man, brother, he, him, son}}\}$. For each word vector obtained from both GloVe and Word2Vec, we can calculate this average cosine distance to the set of anchor words representing the broader concepts of "boy" and "girl".  Importantly, the simplicity of this analytic strategy belies the fact that words, concepts, and people can all be high (or low) in *both* masculine and feminine characteristics, suggesting that this method is a somewhat crude, if convenient, way of conceptualizing gender valence in lexical items.

```{r}

girl_words <- 
  c(
    "woman",
    "girl",
    "sister",
    "she",
    "her",
    "daughter"
  )

boy_words <- 
  c(
    "man",
    "boy",
    "brother",
    "he",
    "him",
    "son"
  )
  

```


```{r message=F}
#Glove embeddings
full_vectors_tbl <- 
  read_csv(fs::path(project_root, "glove_full_50.csv")) 

full_vectors <- as.matrix(full_vectors_tbl %>% select(-rowname))

rownames(full_vectors) <- full_vectors_tbl$rowname
```

```{r}

#w2v <- word2vec(txt, type = "cbow", dim = 50, iter = 20)

w2v_tibble <- read_csv(fs::path(project_root, "full_w2v_300.csv"))

w2v <- 
  w2v_tibble %>% 
  select(-rowname) %>% 
  as.matrix()

rownames(w2v) <- w2v_tibble$rowname

```

```{r}

glove_girl_set <- 
  full_vectors[rownames(full_vectors) %in% girl_words, ,drop = F]

glove_girl_set_sims <- 
  text2vec::sim2(
    x = full_vectors, 
    y = glove_girl_set, 
    method = "cosine", 
    norm = "l2"
  ) %>% 
  as_tibble(rownames = NA) %>%
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(daughter, woman, sister, girl, her, she))) %>% 
  arrange(-avg)

glove_boy_set <- 
  full_vectors[rownames(full_vectors) %in% boy_words, ,drop = F]

glove_boy_set_sims <- 
   text2vec::sim2(
    x = full_vectors, 
    y = glove_boy_set, 
    method = "cosine", 
    norm = "l2"
  ) %>% 
  as_tibble(rownames = NA) %>%
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(son, man, brother, boy, him, he))) %>% 
  arrange(-avg)


```


```{r}

w2v_girl_set_sims <- 
  w2v[girl_words,] %>% 
  word2vec_similarity(
    y = w2v,
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(woman, girl, sister, she, her, daughter))) %>% 
  arrange(-avg)

w2v_boy_set_sims <- 
  w2v[boy_words,] %>% 
  word2vec_similarity(
    y = w2v,
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(man, boy, brother, he, him, son))) %>% 
  arrange(-avg)


```


```{r}
set_similarities_glove <- 
  glove_boy_set_sims %>% 
  select(word, avg_boy = avg) %>% 
  left_join(
    glove_girl_set_sims %>% select(word, avg_girl = avg),
    by = "word"
  ) %>% 
  pivot_longer(
    starts_with("avg"),
    names_to = "gender",
    values_to = "sim_score_glove"
  )

set_similarities_w2v <- 
  w2v_boy_set_sims %>% 
  select(word, avg_boy = avg) %>% 
  left_join(
    w2v_girl_set_sims %>% select(word, avg_girl = avg),
    by = "word"
  ) %>% 
  pivot_longer(
    starts_with("avg"),
    names_to = "gender",
    values_to = "sim_score_w2v"
  )

set_sims_both_models <- 
  set_similarities_glove %>% 
  left_join(
    set_similarities_w2v,
    by = c("word", "gender")
  )

comparison_fig <- 
  set_sims_both_models %>% 
  #filter(sim_score_glove < 0.5) %>% 
  mutate(gender = fct_recode(
    gender,
    `Similarity to 'boy' words` = "avg_boy",
    `Similarity to 'girl' words` = "avg_girl"
  )) %>% 
  ggplot(aes(
    x = sim_score_glove,
    y = sim_score_w2v
  )) +
  geom_hex(bins = c(30, 60)) +
  facet_wrap(~gender) +
  scale_fill_viridis() +
  geom_smooth(method = "loess", color = "red") +
  labs(
    x = "Cosine distance to anchor words (GloVe)",
    y = "Cosine distance\nto anchor words (Word2Vec)") +
  ggthemes::theme_few() +
  theme(axis.title = element_text(size = 16)) +
  theme(strip.text = element_text(size = 14))

ggsave(
  filename = "correlation_comparison_fig.png",
  plot = comparison_fig,
  path = fs::path(project_root, "psy_cos360_paper", "figs"),
  width = 6.5,
  height = 4,
  units = "in"
)

corrplotcaption <- "Hexbin plot showing association between gender valence of individual words vectors in Word2Vec and GloVe. Red lines are LOESS smoothing with 95 percent confidence intervals."
  
```

```{r corr_plot, fig.env = "figure", fig.pos = "h", fig.width=3.5, fig.height=2.0, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = corrplotcaption}

img <- png::readPNG("figs/correlation_comparison_fig.png")
grid::grid.raster(img)

```

To get a rough sense of whether both models are similarly capturing semantic information related to gender, we can plot the average cosine distance to the anchor words from both models on opposing axes (Figure \ref{fig:corr_plot}). In both similarity to the 'girl' anchor words and similarity to the 'boy' anchor words, the models show a slight positive association where the density of points is highest; however, this relationship is not discernible for words with more extreme values of similarity to the anchor words. In addition, a positive correlation between similarity metrics from the two models is more appreciable when measuring individual words' similarity to the 'girl' anchor words (right panel of Figure \ref{fig:corr_plot}) compared to the 'boy' anchor words. The slight positive association seen in Figure \ref{fig:corr_plot}, indicates that while the two models do seem to capture some of the same gender valence information, there is also substantial variation between the models.

## Analysis 2: Correspondence with human ratings of word gender valence

Are word embedding models capturing an aspect of individual words' gender valence that corresponds with human intuitions about how gendered those words are? To examine this question, I computed each word's gender valence with the same strategy as in Analysis 1. 

To examine whether this quantity captures information about words that accords with human intuitions about how "boyish" or "girlish" a word is, I use gender judgments of over 2,000 word types by human raters from Amazon Mechanical Turk, originally collected by @lewis2020might. Each word was rated by approximately 7 participants, and ratings fall on a 1-5 scale, with 1 indicating that a word is as male-typed as possible and 5 indicating that a word is as female-typed as possible. The number of words for which this dataset contains human ratings is far less than the number of word types present in CHILDES, so a correlation between the word-vector similarity quantity previously mentioned and the human ratings is only possible for a small subset of the CHILDES word types.

```{r}

#read in word embeddings from boy-directed and girl-directed speech.

boy_w2v_tibble <- 
  read_csv(fs::path(project_root, "boy_w2v_50.csv"))

boy_w2v <- as.matrix(boy_w2v_tibble %>% select(-rowname))

rownames(boy_w2v) <- boy_w2v_tibble$rowname

girl_w2v_tibble <- 
  read_csv(fs::path(project_root, "girl_w2v_50.csv"))

girl_w2v <- as.matrix(girl_w2v_tibble %>% select(-rowname))

rownames(girl_w2v) <- girl_w2v_tibble$rowname

```

```{r}
#get similarity to boy and girl clusters

#In speech to girls, what are the top similar words to the girl cluster?
girl_w2v_girl_set_sims <- 
  girl_w2v[girl_words,] %>% 
  word2vec_similarity(
    y = girl_w2v,
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(woman, girl, sister, she, her, daughter))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_girls = avg)

#In speech to girls, what are the top similar words to the boy cluster?
girl_w2v_boy_set_sims <- 
  girl_w2v[boy_words,] %>% 
  word2vec_similarity(
    y = girl_w2v,
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(man, boy, brother, he, him, son))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_girls = avg)

#In speech to boys, what are the top similar words to the girl cluster?
boy_w2v_girl_set_sims <- 
  boy_w2v[girl_words,] %>% 
  word2vec_similarity(
    y = boy_w2v,
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(woman, girl, sister, she, her, daughter))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_boys = avg)

#In speech to boys, what are the top similar words to the boy cluster?
boy_w2v_boy_set_sims <- 
  boy_w2v[boy_words,] %>% 
  word2vec_similarity(
    y = boy_w2v,
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(man, boy, brother, he, him, son))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_boys = avg)
```


```{r}

human_judgements <- 
  read_csv(
    file = fs::path(project_root, "human_ratings", "gender_ratings_mean.csv")
  )

human_diffs_girl_directed_speech <- 
  girl_w2v_girl_set_sims %>% 
  select(word, sim_to_girl_words = to_girls) %>% 
  left_join(
    girl_w2v_boy_set_sims %>% select(word, sim_to_boy_words = to_girls),
    by = "word"
  ) %>% 
  mutate(diff = sim_to_girl_words - sim_to_boy_words) %>% 
  inner_join(
    human_judgements %>% select(word, rating = mean),
    by = "word"
  ) %>% 
  select(word, diff, rating) %>% 
  mutate(target = "girl-directed")

human_diffs_boy_directed_speech <- 
  boy_w2v_girl_set_sims %>% 
  select(word, sim_to_girl_words = to_boys) %>% 
  left_join(
    boy_w2v_boy_set_sims %>% select(word, sim_to_boy_words = to_boys),
    by = "word"
  ) %>% 
  mutate(diff = sim_to_girl_words - sim_to_boy_words) %>% 
  inner_join(
    human_judgements %>% select(word, rating = mean),
    by = "word"
  ) %>% 
  select(word, diff, rating) %>% 
  mutate(target = "boy-directed")

full_human_diffs <- 
  bind_rows(
    human_diffs_boy_directed_speech, 
    human_diffs_girl_directed_speech
  ) 

full_human_diff_plot <- 
  full_human_diffs %>% 
  ggplot(aes(rating, diff)) +
  facet_wrap(~target) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", color = "red") +
  viridis::scale_fill_viridis() +
  coord_cartesian(ylim = c(-.3, .3)) +
  labs(
    x = "Human ratings (1 = most masculine, 5 = most feminine)",
    y = "Word embedding difference score\n(Higher = more girl-like)"
  ) +
  ggthemes::theme_few() +
  theme(axis.title = element_text(size = 16)) +
  theme(strip.text = element_text(size = 14))


ggsave(
  filename = "full_human_diff_plot.png",
  plot = full_human_diff_plot,
  path = fs::path(project_root, "psy_cos360_paper", "figs"),
  width = 6.5,
  height = 4,
  units = "in"
)

humanplotcap <- "Scatter plot showing individual words' gender valence, as determined by human raters (horizontal axis) against word embedding difference score (vertical axis). Lines are best linear fits with associated 95 percent confidence intervals."



```


```{r human_plot, fig.env = "figure", fig.pos = "h", fig.width=3.5, fig.height=2.15, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = humanplotcap}

img <- png::readPNG("figs/full_human_diff_plot.png")
grid::grid.raster(img)

```

```{r}
#correlations

boy_cor <- cor(
  human_diffs_boy_directed_speech$diff,
  human_diffs_boy_directed_speech$rating
) %>% round(4)

girl_cor <- cor(
  human_diffs_girl_directed_speech$diff,
  human_diffs_girl_directed_speech$rating
) %>% round(4)
```

```{r}
cdi_words <- 
  c("airplane", "boat", "car", "ball", "book", "meat", "duck", "game", "hat", "broom", "tray", "necklace", "comb", "towel", "show", "mop", "bed", "sock", "plate", "trash", "beach", "oven", "stair", "flag", "star", "swing", "school", "sky", "party", "friend", "mommy", "person", "carry", "chase", "finish", "fit", "hug", "listen", "like", "pretend", "rip", "shake", "taste", "think", "wish")

cds_cdi_to_feminine <- 
  w2v[girl_words,] %>% 
  word2vec_similarity(
    y = w2v[cdi_words,],
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(woman, girl, sister, she, her, daughter))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_girls = avg) %>% 
  select(word, to_girls)

cds_cdi_to_masculine <- 
  w2v[boy_words,] %>% 
  word2vec_similarity(
    y = w2v[cdi_words,],
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(man, boy, brother, he, him, son))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_boys = avg) %>% 
  select(word, to_boys)

gds_cdi_to_feminine <- 
  girl_w2v[girl_words,] %>% 
  word2vec_similarity(
    y = girl_w2v[cdi_words,],
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(woman, girl, sister, she, her, daughter))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_girls = avg) %>% 
  select(word, to_girls) 

gds_cdi_to_masculine <- 
  girl_w2v[boy_words,] %>% 
  word2vec_similarity(
    y = girl_w2v[cdi_words,],
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(man, boy, brother, he, him, son))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_boys = avg) %>% 
  select(word, to_boys)

bds_cdi_to_feminine <- 
  boy_w2v[girl_words,] %>% 
  word2vec_similarity(
    y = boy_w2v[cdi_words,],
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(woman, girl, sister, she, her, daughter))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_girls = avg) %>% 
  select(word, to_girls)

bds_cdi_to_masculine <- 
  boy_w2v[boy_words,] %>% 
  word2vec_similarity(
    y = boy_w2v[cdi_words,],
    type = "cosine"
  ) %>% 
  t() %>% 
  as_tibble(rownames = NA) %>% 
  rownames_to_column("word") %>% 
  rowwise() %>% 
  mutate(avg = mean(c(man, boy, brother, he, him, son))) %>% 
  arrange(-avg) %>% 
  rowid_to_column() %>% 
  rename(to_boys = avg) %>% 
  select(word, to_boys)

cds_diff <- 
  cds_cdi_to_feminine %>% 
  left_join(cds_cdi_to_masculine, by = "word") %>% 
  mutate(diff = to_girls - to_boys, dataset = "Overall") %>% 
  arrange(-diff)

gds_diff <- 
  gds_cdi_to_feminine %>% 
  left_join(gds_cdi_to_masculine, by = "word") %>% 
  mutate(diff = to_girls - to_boys, dataset = "Girl-directed speech") %>% 
  arrange(-diff)

bds_diff <- 
  bds_cdi_to_feminine %>% 
  left_join(gds_cdi_to_masculine, by = "word") %>% 
  mutate(diff = to_girls - to_boys, dataset = "Boy-directed speech") %>% 
  arrange(-diff)

all_diff <- 
  bind_rows(cds_diff, gds_diff, bds_diff) %>% 
  pivot_longer(cols = starts_with("to"), names_to = "anchors", values_to = "cosine_sim_to_anchors")

all_diff$word <- fct_reorder(all_diff$word, all_diff$diff)

cdi_plot <- 
  all_diff %>% 
  mutate(
    word = fct_reorder(word, diff),
    dataset = fct_relevel(dataset, "Overall")
  ) %>% 
  ggplot(aes(diff, fct_inorder(word))) +
  facet_wrap(~dataset) +
  geom_point() + 
  theme_minimal() +
  labs(
    x = "(Mean cosine distance from 'girl' anchors) - (Mean cosine distance from 'boy' anchors)"
  ) +
  theme(axis.title.y = element_blank()) +
  theme(axis.text = element_text(size = 8)) +
  theme(strip.text = element_text(size = 10))

ggsave(
  filename = "cdi_plot.png",
  plot = cdi_plot,
  path = fs::path(project_root, "psy_cos360_paper", "figs"),
  width = 6.5,
  height = 5.5,
  units = "in"
)

cdicap <- "Gender valence of word vector representations of 45 words from the MacArthur-Bates Communicative Development Inventory Short Form. Horizontal axis represents, for each word on the vertical axis, the difference between (a) the mean cosine similarity to the 'girl' anchors and (b) mean cosine similarity to the set of 'boy' anchors."
```

```{r cdiplot, fig.env = "figure*", fig.pos = "h", fig.width=6.5, fig.height=5.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = cdicap}

img <- png::readPNG("figs/cdi_plot.png")
grid::grid.raster(img)

```

Additionally, for this analysis, two separate Word2Vec models were trained: one on speech from CHILDES that was directed at male children, and another on speech from CHILDES directed at female children, to examine if communication to children of different genders shows more or less correspondence with human intuitions about gender valence. Figure \ref{fig:cdiplot} shows how these two models, in addition to the one trained on all child-directed utterances (regardless of the child's gender), represent the gender valence of 45 nouns and verbs selected from the MacArthur-Bates Communicative Development Inventory Short Form [@fenson2000short], a language development assessment used with children between the ages of 16 and 30 months. One noticeable pattern from Figure \ref{fig:cdiplot} is that word vectors with higher male-valence - at least in the model trained on all of the child-directed utterances - tend to be modes of transportation, while more female-valenced words tend to be articles of clothing, jewelry, and domestic household items.

The results of the word vector correlations with human ratings are displayed in Figure \ref{fig:human_plot}. Word embedding models trained either on only speech to girls or speech to boys both capture information about individual words' gender valence that shows a reliable positive association with human ratings of a word's gender valence ($r_\text{girl-directed speech} =$ `r girl_cor`; $r_\text{boy-directed speech}=$ `r boy_cor`). The correlations between human judgments and word embedding difference scores underscore two points. First, the gendered nature of certain words, as intuited by human raters, is discernible in the distributional semantics of language directed to children. Second, according to the (very coarse) correlational measure described above, this gender valence of individual words in child-directed speech does not look radically different between speech directed to boys vs. girls, though other analytic strategies could potentially still detect differences in how gendered language is communicated to boys vs. girls, if such differences exist.


## Analysis 3: Specific gender stereotypes

Does child-directed speech in the CHILDES corpus contain specific gender stereotypes? In Analysis, 3, I examined whether word vector representations derived from CHILDES encode semantic information that encapsulates three particular stereotypes which have been the subject of prior research in social psychology: (1) Female as home-oriented, male as work-oriented; (2) Female as good, male as bad; and (3) Female as oriented towards reading and language, and male as math-oriented. To assess the extent to which these stereotypes might surface in the distributional semantics of the CHILDES corpus, the Word Embedding Association Test [WEAT; @caliskan2017semantics] was conducted. The goal of the WEAT is to quantify stereotypical biases in large bodies of text in a manner analogous to how the Implicit Association Test quantifies implicit bias in people [@greenwald1998measuring]. 

```{r xtable, results='asis'}

attributes <- 
  tribble(
    ~Attribute, ~Words,
    "Home", "{family, children, home, cousin, parent, wedding}",
    "Work", "{job, work, money, office, business, desk}",
    "Language", "{book, read, write, letter, spell, story}",
    "Math", "{number, count, sort, size, shape, different}",
    "Good", "{good, happy, gift, sunshine, heaven}",
    "Bad", "{bad, awful, sick, trouble, hurt}"
  )

kbl(
  attributes, 
  booktabs = TRUE,
  caption = "Words for each attribute set."
) %>% 
  column_spec(2, width = "2in")
  

```

Defined more precisely, as in Caliskan (2017), we can consider two equally-sized sets of anchor words $X$ and $Y$ (for example, the sets corresponding to the anchor words for 'boy' and 'girl') and two sets of attribute words $A$ and $B$ (for example, sets of words corresponding to the attributes 'good at reading' and 'good at math'). The effect size generated by the WEAT is 
$$\frac{\text{mean}_{x \in X}s(x, A, B) - \text{mean}_{y \in Y}s(y, A, B)}{\text{std dev}_{w \in X \cup Y}s(w, A, B)}$$
where
$$s(w, A, B) = \text{mean}_{a \in A}\text{cos}(\vec{w}, \vec{a}) - \text{mean}_{b \in B}\text{cos}(\vec{w}, \vec{b})$$

```{r}


X <- c("woman", "girl", "sister", "she", "her", "daughter")

Y <- c("man", "boy", "brother", "he", "him", "son")

home <- c("family", "parent", "children", "home", "cousin", "wedding")

work <- c("job", "work", "money", "office", "business", "desk")

language <- c("book", "read", "write", "story", "letter", "spell")

math <- c("number", "count", "sort", "size", "shape", "different")

good <- c("good", "happy", "gift", "sunshine", "heaven")

bad <- c("bad", "awful", "sick", "trouble", "hurt")

  
calculate_weat <- function(mat, X, Y, A, B) {
  
  x_a <- vector("list")
  
  y_a <- vector("list")
  
  for (x in X) {
    this_x_diffs <- vector("list")
    for (a in A) {
      this_sim = word2vec_similarity(mat[x, ], mat[a, ], type = "cosine")
      this_x_diffs <- append(this_x_diffs, this_sim)
    }
    this_avg_sim <- mean(unlist(this_x_diffs))
    x_a <- append(x_a, this_avg_sim)
  }
  
  for (y in Y) {
    this_x_diffs <- vector("list")
    for (a in A) {
      this_sim = word2vec_similarity(mat[y, ], mat[a, ], type = "cosine")
      this_y_diffs <- append(this_x_diffs, this_sim)
    }
    this_avg_sim <- mean(unlist(this_y_diffs))
    y_a <- append(y_a, this_avg_sim)
  }
  
  numerator <- mean(unlist(x_a)) - mean(unlist(y_a))
  
  denom <- sd(unlist(c(x_a, y_a)))
  
  return(numerator/denom)
  
}

overall_home_work <- calculate_weat(w2v, X, Y, home, work)

to_girls_home_work <- calculate_weat(girl_w2v, X, Y, home, work)

to_boys_home_work <- calculate_weat(boy_w2v, X, Y, home, work)

overall_language_math <- calculate_weat(w2v, X, Y, language, math)

to_girls_language_math <- calculate_weat(girl_w2v, X, Y, language, math)

to_boys_language_math <- calculate_weat(boy_w2v, X, Y, language, math)

overall_good_bad <- calculate_weat(w2v, X, Y, good, bad)

to_girls_good_bad <- calculate_weat(girl_w2v, X, Y, good, bad)

to_boys_good_bad <- calculate_weat(boy_w2v, X, Y, good, bad)


```


```{r}

calculate_null <- function(mat, X, Y, A, B, n = 1000) {
  effects_vec <- vector("list")
  for (i in 1:n) {
    xy <- append(X, Y)
    shuffled_xy <- sample(xy)
    new_x <- shuffled_xy[1:6]
    new_y <- shuffled_xy[7:12]
    effects_vec <- append(effects_vec, calculate_weat(mat, new_x, new_y, A, B))
  }
  return(effects_vec %>% unlist)
}

overall_home_work_null <- calculate_null(w2v, X, Y, home, work)

to_girls_home_work_null <- calculate_null(girl_w2v, X, Y, home, work)

to_boys_home_work_null <- calculate_null(boy_w2v, X, Y, home, work)

overall_language_math_null <- calculate_null(w2v, X, Y, language, math)

to_girls_language_math_null <- calculate_null(girl_w2v, X, Y, language, math)

to_boys_language_math_null <- calculate_null(boy_w2v, X, Y, language, math)

overall_good_bad_null <- calculate_null(w2v, X, Y, good, bad)

to_girls_good_bad_null <- calculate_null(girl_w2v, X, Y, good, bad)

to_boys_good_bad_null <- calculate_null(boy_w2v, X, Y, good, bad)


```


```{r}

#Building a tibble for the plot.

weat_plot_tibble <- 
  tribble(
    ~stereotype, ~target, ~effect_size, ~sd,
    "home_work", "overall", overall_home_work, sd(overall_home_work_null),
    "home_work", "girls", to_girls_home_work, sd(to_girls_home_work_null),
    "home_work", "boys", to_boys_home_work, sd(to_boys_home_work_null),
    "good_bad", "overall", overall_good_bad, sd(overall_good_bad_null),
    "good_bad", "girls", to_girls_good_bad, sd(to_girls_good_bad_null),
    "good_bad", "boys", to_boys_good_bad, sd(to_boys_good_bad_null),
    "language_math", "overall", overall_language_math, sd(overall_language_math_null),
    "language_math", "girls", to_girls_language_math, sd(to_girls_language_math_null),
    "language_math", "boys", to_boys_language_math, sd(to_boys_language_math_null)
  ) %>% 
  mutate(
    errorbar_lower = effect_size - (1.96 * sd),
    errorbar_upper = effect_size + (1.96 * sd)
  )

weat_plot <- 
  weat_plot_tibble %>% 
  mutate(stereotype = fct_recode(
    stereotype,
    "Female-Good/\nMale-Bad" = "good_bad",
    "Female-Home/\nMale-Work" = "home_work",
    "Female-Language/\nMale-Math" = "language_math"
  )) %>% 
  ggplot(aes(x= stereotype, fill = target)) +
  geom_col(position = "dodge", aes(y = effect_size)) +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  geom_errorbar(
    position = position_dodge(0.9),
    aes(ymin = errorbar_lower, ymax = errorbar_upper), 
    width = 0.3
  ) +
  scale_fill_viridis_d(option = "D") +
  ggthemes::theme_few() + 
  labs(y = "Implicit Bias effect size (Cohen's D)", fill = "Corpus") +
  theme(axis.title.x = element_blank()) +
  theme(legend.position = "bottom")

ggsave(
  filename = "weat_plot.png",
  plot = weat_plot,
  path = fs::path(project_root, "psy_cos360_paper", "figs"),
  width = 4,
  height = 4,
  units = "in"
)

weatcap <- "WEAT effect sizes for each stereotype. Error bars are 95 percent confidence intervals on effect size estimates, generated by randomly shuffling target word vector labels and simulating the WEAT effect size 1000 times."
```


```{r weatplot, fig.env = "figure", fig.pos = "h", fig.width=3.5, fig.height=3.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = weatcap}

img <- png::readPNG("figs/weat_plot.png")
grid::grid.raster(img)

```

For each of the three previously mentioned stereotypes, a WEAT score was computed using 3 Word2Vec embeddings: one trained only on boy-directed speech, one directed only on girl-directed speech, and one trained on both. The words in each attribute set are displayed in Table 1 and were borrowed from Lewis et al. (2020). 

Results from the WEAT test are displayed in Figure \ref{fig:weatplot}. Ninety-five percent confidence intervals were generated for WEAT effect size estimates by randomly shuffling which target words corresponded with the "boy" concept and which with the "girl" concept, and subsequently computing a WEAT effect size with the shuffled word labels 1,000 times to obtain an empirical null distribution (following the strategy used in Charlesworth et al., 2020). For the Female-Good/Male-Bad stereotype, WEAT computed on the CHILDES data showed an effect associating 'girl' with 'bad' and 'boy' with 'good', which is in the opposite direction from what is considered stereotypical in prior literature [@cvencek2011math]. However, only the effect size generated from speech directed to boys had an effect size estimate whose 95% confidence interval did not include zero. The Female-Home/Male-Work stereotype only showed an effect whose 95% interval did not include zero with a word embedding model trained on speech to both boys and girls; the boy-directed and girl-directed corpora yielded smaller and less robust effects, though all were in the stereotypically-expected direction. The Female-Reading/Male-Math stereotype showed, overall, the largest effect sizes across all three training sets, with all of the three having confidence intervals that did not include zero. 

As demonstrated in Figure \ref{fig:weatplot}, the results from WEAT on the three CHILDES corpora (all child-directed speech, boy-directed speech, and girl-directed speech) are somewhat mixed. Many of the WEAT calculations generated relatively large effect sizes, but large variance in the empirical null distribution of effect sizes, resulting in wide confidence intervals on the effect size estimates, constrains the interpretation of above data. Nonetheless, some gender stereotypes are discernible in the distributional semantics of the CHILDES corpus, particularly the stereotype that associates boys with math and girls with reading/language.

# Discussion

In this paper, I explored the potential presence of gender stereotypes in the distributional semantics of child-directed speech across three analyses using the CHILDES corpus [@macwhinney2000childes]. In the first analysis, I compared how two commonly-used word embedding models, GloVe and Word2Vec, capture the gender valence of the same words. Figure \ref{fig:corr_plot} shows that the measure of gender valence that the two models generate are (at most) modestly correlated. In the second analysis, I obtained two additional sets of word vector representations by training Word2Vec on (a) just speech to girls and (b) just speech to boys (Figure \ref{fig:cdiplot}), and correlated the models' estimates of individual words' gender valence to human-generated judgments(Figure \ref{fig:human_plot}). The correlation seen in Figure \ref{fig:human_plot} analysis demonstrates that Word2Vec, trained on child-directed speech, to some extent captures humans' intuitions about the gender valence of particular words. In the third analysis, I used an existing measure of gender stereotyping in large bodies of text, the Word Embedding Association Test, to quantify whether specific gender stereotypes are present in child-directed speech. Figure \ref{fig:weatplot} illustrates that specific gender stereotypes were captured by Word2Vec, but inconsistently so. The Female-Language/Male-Math stereotype appeared to be the most robust effect, while the Female-Good/Male-Bad effect was in the opposite direction from the predicted result based on past literature. Moreover, across all three stereotypes, the empirical null distribution generated by randomly permuting which anchor words referred to the 'boy' and 'girl' categories had large variance, constraining a strong interpretation of the effect sizes observed from the WEAT.

Given the results seen here, it is difficult to clearly explain whether and how language directed to boys differs from language directed to girls in terms of how much gender stereotypical information is communicated to children. The discrepant pattern between the panels of Figure \ref{fig:cdiplot} indicates that there was substantial variability in individual words' gender valence as captured by Word2Vec depending on whether the model was trained on girl-directed vs. boy-directed speech. In future work, one potential approach to better understand whether the speech to boys vs. girls contains differing stereotype content is to examine the frequency of certain words that have a strong gender valence in girl-directed and boy-directed speech, similar to the approach in @braginsky2016gender. A qualitative analysis of when and in what context these very gendered words appear might also explain potential differences in boy-directed vs. girl-directed speech. 

Training a word embedding model on child-directed utterances from CHILDES - and then subsequently trying to use those word embeddings to gain insight into parent-child communication - carries some inherent limitations. First, CHILDES contains conversations between parents and children in which children actively shape the communicative interaction. In this paper, I excised all words spoken by children when preparing the training data; in doing so, some information is lost about how highly gendered speech enters into communication between caregivers and children. Training a word embedding model on the text from one side of a two-sided conversation is also methodologically problematic, since it forces some constraints on the definition of what it means for one word to be "in the context" of another. In this paper, I concatenated all of the adult utterances, which forces some words to be side-by-side in the training data when, in actual conversation, they were separated by a child utterance. One potential solution is to adjust how the model considers which words are in the context of any other given word, by not extending the context window beyond the immediate conversational turn a word appears in. This is also an imperfect solution, however, because live conversation often includes distinct utterances that are interrupted by other speakers or are distributed across several conversational turns.  

These methodological challenges notwithstanding, there are several interesting directions for future research that employ similar methodology to the current work. This paper did not consider how language to children changes across development. Might caregivers communicate about gender differently depending on the age of their child? Examining whether the gender valence of particular word vectors changes systematically over development could offer some insight on this question. Additionally, comparing the vector-space representation of words in CHILDES with pre-trained Word2Vec and GloVe vectors would potentially help clarify how gendered we would expect certain words to be in a much larger, more linguistically and contextually heterogeneous corpus, and thus provide a useful reference point for the gendered information we see captured in the word embedding models trained on CHILDES. Moreover, state-of-the-art language models are able to encode semantic information of significantly more complexity than the tools used in these analyses, including sentence-level contextual information [e.g., @devlin2018bert]. Investigating the extent to which these models learn gender biases and stereotypes based on child-directed speech is an exciting possibility for future research, but would require substantially larger corpora of child-directed speech than CHILDES.

Returning to the question of how exactly children learn gender biases and stereotypes, my results here suggest that child-directed speech may indeed subtly shape children's gendered associations with traits, activities, and objects, simply by virtue of the distributional semantics of the speech they hear. The results here are, however, also mixed; future investigations with larger datasets - and more power to understand how caregivers' communication about gender might vary across development - are needed to understand the extent to which distributional semantics of child-directed speech shape gender biases and stereotypes.

# Acknowledgements

Thanks to Dan Friedman and Zaid Zada for guidance on this project, and to Kyle MacDonald for his RMarkdown CogSci tools.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
